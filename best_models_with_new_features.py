# -*- coding: utf-8 -*-
"""best-models-with-new-features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OztiegKvTGC_mhR_RVLgwTxjydxElqr8
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import transforms
import seaborn as sns
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from catboost import CatBoostClassifier
from sklearn.preprocessing import OneHotEncoder 
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFECV
from sklearn import  linear_model
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import BaggingClassifier
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
# %matplotlib inline

"""##In this notebook we add two new features to our database and extend our previous work on feature engineering."""

porto=pd.read_csv("/content/drive/My Drive/porto-seguro/train.csv");

porto.target.plot.hist() #target imbalance

portowtnan=porto.replace(-1,np.nan)

portowtnan['missingmark']=portowtnan.isnull().sum(1)

portowtnan.replace(np.nan,-1,inplace=True)

portowtnan.info()

portowtnan.drop(columns=['ps_car_03_cat','id'],axis=1,inplace=True) #drop features with more than 60% null values
target=portowtnan['target']
portowtnan.drop(columns=['target'],axis=1,inplace=True)
cols=portowtnan.columns

portowtnan

portoclean=portowtnan
portoclean.head()

portocalc=[cols for cols in portoclean.columns if cols.find("calc")!=-1 and cols.find("13")==-1] #drop calc
portocalc

portoclean.drop(portocalc,axis=1,inplace=True)
portoclean

portobin=[cols for cols in portoclean.columns if cols.find("bin")!=-1 ]
portocat=[cols for cols in portoclean.columns if cols.find("cat")!=-1 ]
portonormal=[cols for cols in portoclean.columns if cols.find("bin")==-1 & cols.find("cat")==-1 ]

portobin

prbin=portoclean.loc[:,portobin]
prbin

selected=portobin
selected

portocat

prcat=portoclean.loc[:,portocat]
prcat

"""##New features and their impact on insurance claims"""

elem=[cols for cols in prcat.columns if cols.find("car")!=-1 ]
df=pd.DataFrame()
df['target']=target
df['car']=np.zeros(len(prcat))
for col in elem:
  df['car']=df['car']+prcat[col]
df['car']=(df['car'])**.05
df.groupby('car')['target'].sum().plot.bar()

"""the new car column is showing significant impact on the distribution of insurance claims.There we add this to our dataset"""

def calculatefreq(selectcat,df): #frequency encoder 
  for col in selectcat:
    newcol='%s_freq'%(col)    
    m=portoclean[col].astype('category').value_counts(dropna=False).to_dict()
    df[newcol]=df[col].map(m)
  elem=[cols for cols in selectcat if cols.find("car")!=-1 ]  
  df['car']=np.zeros(len(df)) #add logic to get car column
  for col in elem:
    df['car']=df['car']+df[col]
  df['car']=(df['car'])**.05
    
  return(df)

selectcat=portocat

prfreq=calculatefreq(selectcat,prcat)
prfreq

prcat=prfreq
selectcat=prfreq.columns

prcat.columns

portonormal

prnor=portoclean.loc[:,portonormal]
prnor

selectednorm=portonormal

features_train=pd.concat([prbin,prcat,prnor],join='outer',axis=1)
selectedfeatures=pd.concat([pd.Series(selected),pd.Series(selectcat),pd.Series(selectednorm)],axis=0,ignore_index=True)

def featureengineering(df): #feature engineering same as before
  top=['ps_car_14','ps_car_15','ps_reg_03','ps_ind_15']
  i=0
  for col in top:
    new='%d'%i
    new1='%d_interac'%i   
    m=portoclean.groupby(col)['ps_car_13'].mean().to_dict()      
    df[new]=df[col].map(m)
    df[new1]=(df['ps_car_13']*df[col])   
    i=i+1  
  tenco=pd.DataFrame()
  tenco['target']=target
  cat=['ps_car_11_cat','ps_car_06_cat']
  for col in cat:
   tenco[col]=prcat[col]
   m=(tenco.groupby(col)['target'].mean()).to_dict()
   newcol='%s_new'%col
   df[newcol]=df[col].map(m)
  listcol=[cols for cols in portoclean.columns if cols.find("ind")!=-1 ] #adding another feature ind 
  df['ind']=np.zeros(len(df))    
  for col in listcol:
    df['ind']=df['ind']+df[col]  
  df['ind']=df['ind']**2
  return df

features_train=featureengineering(features_train.copy())
features_train

def ginic(actual, pred):
    actual = np.asarray(actual) 
    n = len(actual)
    a_s = actual[np.argsort(pred)]
    a_c = a_s.cumsum()
    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0
    return giniSum / n
 
def gini_normalizedc(a, p):
    if p.ndim == 2:
        p = p[:,1] 
    return ginic(a, p) / ginic(a, a)

xgb1 = xgb.XGBClassifier(tree_method='gpu_hist',learning_rate=0.01, 
                            n_estimators=1600, 
                            max_depth=6,                            
                            gamma=0.5,
                            subsample=.8,
                            colsample_bytree=0.3,
                            objective= 'binary:logistic',                            
                            scale_pos_weight=1,
                            reg_alpha = 1,
                            reg_lambda = 3,
                            n_jobs=-1,
                            seed=49)  #0.27405 0.29249

#0.27826 0.29236
lg=lgb.LGBMClassifier(boosting_type='gbdt',learning_rate=.01,max_depth=6,n_estimators=1400,colsample_bytree=.3,reg_alpha=3, reg_lambda=3)

#0.27624 0.28968
model = CatBoostClassifier(iterations=2500,
learning_rate=0.01,
depth=8,
)

cv = StratifiedKFold(n_splits=2,shuffle=True,random_state=1)
for train_ix,test_ix in cv.split(features_train,target):
  X_train, X_test = features_train.iloc[train_ix,:], features_train.iloc[test_ix,:]
  y_train, y_test = target[train_ix],target[test_ix]
  lg.fit(X_train,y_train)
  y_prob=lg.predict_proba(X_test)
  score=gini_normalizedc(y_test,y_prob)
  print(score)

modelgb=model.fit(features_train,target)

fig, ax = plt.subplots(1,1,figsize=(15,15))
lgb.plot_importance(lg,ax=ax)

testdata=pd.read_csv('/content/drive/My Drive/porto-seguro/test.csv')

id=testdata['id']

testdata.replace(-1,np.nan,inplace=True)

testdata['missingmark']=testdata.isnull().sum(1)
testdata.replace(np.nan,-1,inplace=True)

testdata.drop(columns=portocalc,axis=1,inplace=True)

testdata.drop(columns=['id','ps_car_03_cat'],axis=1,inplace=True)

testdata.head()

testbin=testdata.loc[:,selected].copy()
testbin

testnor=testdata.loc[:,selectednorm].copy()
testnor

testcatfreq=calculatefreq(portocat,testdata)

testcat=testcatfreq.loc[:,selectcat].copy()
testcat

testcat.nunique()

test1=pd.concat([testbin,testcat,testnor],join='outer',axis=1)
test1

test1=featureengineering(test1.copy())

test1

predictionprob1=pd.DataFrame(modelgb.predict_proba(test1))
predictionprob1

predictionfinal=pd.DataFrame(id,columns=['id'])
predictionfinal['target']=predictionprob1.iloc[:,1]
predictionfinal

predictionfinal.to_csv('prediction.csv',index=False)