# -*- coding: utf-8 -*-
"""feature-engineering-portoproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sK8Xd4ewq5XRMOF5KnqM1MYRC9Kezhoo
"""

!pip3 install catboost

"""##Feature Engineering with stacking classifier,xgboost,lightgbm and catboost"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from numba import jit
import matplotlib.pyplot as plt
from matplotlib import transforms
import seaborn as sns
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder 
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFECV
from sklearn import  linear_model
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import BaggingClassifier
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
# %matplotlib inline

porto=pd.read_csv("/content/drive/My Drive/porto-seguro/train.csv");

porto.target.plot.hist() #target imbalance

"""#Feature Engineering- adding missing mark column to dataset"""

portowtnan=porto.replace(-1,np.nan)

portowtnan['missingmark']=portowtnan.isnull().sum(1) #through eda we came to the conclusion that missing values are affecting claim percentage therefore to make that relationship more explicit we added missing mark column

portowtnan.replace(np.nan,-1,inplace=True)

portowtnan.info() #nan are affecting the claim percentage thats why they are not imputed, for better prediction

portowtnan.drop(columns=['ps_car_03_cat','id'],axis=1,inplace=True) #dropping columns with more than 50% null values
target=portowtnan['target']
portowtnan.drop(columns=['target'],axis=1,inplace=True)
cols=portowtnan.columns

portowtnan

portoclean=portowtnan
portoclean.head()

portocalc=[cols for cols in portoclean.columns if cols.find("calc")!=-1 and cols.find("13")==-1]
portocalc

portoclean.drop(portocalc,axis=1,inplace=True) #dropping calc because they are neutral and leading to poor performance on test data prediction on the leaderboards and in cross validation
portoclean

"""##Database seggregation"""

portobin=[cols for cols in portoclean.columns if cols.find("bin")!=-1 ]
portocat=[cols for cols in portoclean.columns if cols.find("cat")!=-1 ]
portonormal=[cols for cols in portoclean.columns if cols.find("bin")==-1 & cols.find("cat")==-1 ]

portobin

prbin=portoclean.loc[:,portobin]
prbin

selected=portobin
selected

portocat

prcat=portoclean.loc[:,portocat]
prcat

"""##Adding Frequency encoding to categoricals"""

def calculatefreq(selectcat,df): #frequency encoding function
  for col in selectcat:
    newcol='%s_freq'%(col)    
    m=portoclean[col].astype('category').value_counts(dropna=False).to_dict()
    df[newcol]=df[col].map(m)    
  return(df)

selectcat=portocat

prfreq=calculatefreq(selectcat,prcat)
prfreq

prcat=prfreq
selectcat=prfreq.columns

prcat.nunique()

prcat.columns

portonormal

prnor=portoclean.loc[:,portonormal]
prnor

selectednorm=portonormal

features_train=pd.concat([prbin,prcat,prnor],join='outer',axis=1) #joining back the dataset
selectedfeatures=pd.concat([pd.Series(selected),pd.Series(selectcat),pd.Series(selectednorm)],axis=0,ignore_index=True)

features_train.head()

selectedfeatures

features_train.columns

"""##Major Feature Engineering"""

def featureengineering(df):
  top=['ps_car_14','ps_car_15','ps_reg_03','ps_ind_15']  #top features picked from lgb feature importance
  i=0
  for col in top:
    new='%d'%i
    new1='%d_interac'%i    
    m=portoclean.groupby(col)['ps_car_13'].mean().to_dict()    #aggregate features
    df[new]=df[col].map(m)
    df[new1]=df['ps_car_13']*df[col]        #interaction features
    i=i+1  
  tenco=pd.DataFrame()
  tenco['target']=target
  cat=['ps_car_11_cat','ps_car_06_cat'] #adding target-encoding to important categorical variables to get boost in prediciton accuracy
  for col in cat:
   tenco[col]=prcat[col]
   m=(tenco.groupby(col)['target'].mean()).to_dict()
   newcol='%s_new'%col
   df[newcol]=df[col].map(m)

  return df

features_train=featureengineering(features_train.copy())
features_train

def ginic(actual, pred):  #gini score calculator
    actual = np.asarray(actual) 
    n = len(actual)
    a_s = actual[np.argsort(pred)]
    a_c = a_s.cumsum()
    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0
    return giniSum / n
 
def gini_normalizedc(a, p):
    if p.ndim == 2:
        p = p[:,1] 
    return ginic(a, p) / ginic(a, a)

xgb1 = xgb.XGBClassifier(tree_method='gpu_hist',learning_rate=0.01, 
                            n_estimators=1400, 
                            max_depth=6,                            
                            gamma=0.5,
                            subsample=.8,
                            colsample_bytree=0.3,
                            objective= 'binary:logistic',                            
                            scale_pos_weight=1,
                            reg_alpha = 1,
                            reg_lambda = 3,
                            n_jobs=-1,
                            seed=49) #.29365 best performing xgb

lg=lgb.LGBMClassifier(boosting_type='gbdt',learning_rate=.01,max_depth=6,n_estimators=1400,colsample_bytree=.3,reg_alpha=3, reg_lambda=3)

model = CatBoostClassifier(iterations=2500,
learning_rate=0.01,
depth=8,
)

estimators = [('xgb',xgb1),('lgb',lg),('cat',model)]
clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(C=1) ) #.29369 best performing stacking classifier

cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1) #cross validation
for train_ix,test_ix in cv.split(features_train,target):
  X_train, X_test = features_train.iloc[train_ix,:], features_train.iloc[test_ix,:]
  y_train, y_test = target[train_ix],target[test_ix]
  xgb1.fit(X_train,y_train)
  y_prob=xgb1.predict_proba(X_test)
  score=gini_normalizedc(y_test,y_prob)
  print(score)

modelgb=clf.fit(features_train,target)

fig, ax = plt.subplots(1,1,figsize=(15,15))
xgb.plot_importance(xgb1,ax=ax)

testdata=pd.read_csv('/content/drive/My Drive/porto-seguro/test.csv')

id=testdata['id']

testdata.replace(-1,np.nan,inplace=True)

testdata['missingmark']=testdata.isnull().sum(1)
testdata.replace(np.nan,-1,inplace=True)

testdata.drop(columns=['ps_car_03_cat'],axis=1,inplace=True)

testdata.drop(columns=['id'],axis=1,inplace=True)

testdata.head()

testbin=testdata.loc[:,selected].copy()
testbin

testnor=testdata.loc[:,selectednorm].copy()
testnor

testcatfreq=calculatefreq(portocat,testdata)

testcat=testcatfreq.loc[:,selectcat].copy()
testcat

testcat.nunique()

test1=pd.concat([testbin,testcat,testnor],join='outer',axis=1)
test1

test1=featureengineering(test1.copy())

test1

predictionprob1=pd.DataFrame(modelgb.predict_proba(test1))
predictionprob1

predictionfinal=pd.DataFrame(id,columns=['id'])
predictionfinal['target']=predictionprob1.iloc[:,1]
predictionfinal

predictionfinal.to_csv('prediction.csv',index=False)