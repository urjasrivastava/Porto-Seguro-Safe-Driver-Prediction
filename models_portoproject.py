# -*- coding: utf-8 -*-
"""models-portoproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IabIrilao6qY54t3sfOR3W9tj5f_aYgo
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import transforms
import seaborn as sns
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFECV
from sklearn import  linear_model
from sklearn.model_selection import train_test_split,StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
import lightgbm as lgb
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import AdaBoostClassifier
# %matplotlib inline

"""###Trying Logistic Regression,Bagging Classifier, Gausian Naive Bayes and Random Forest on the dataset without any imputation"""

porto=pd.read_csv("/content/drive/My Drive/porto-seguro/train.csv");

porto.target.plot.hist() #target imbalance

lis=[cols for cols in porto.columns if cols.find('calc')!=-1] #dropping of calc features

porto.drop(columns=['ps_car_03_cat','id'],axis=1,inplace=True) #dropping of features with most null values
porto.drop(columns=lis,axis=1,inplace=True)
target=porto['target']
porto.drop(columns=['target'],axis=1,inplace=True)
cols=porto.columns

def ginic(actual, pred):
    actual = np.asarray(actual) 
    n = len(actual)
    a_s = actual[np.argsort(pred)]
    a_c = a_s.cumsum()
    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0
    return giniSum / n
 
def gini_normalizedc(a, p):
    if p.ndim == 2:
        p = p[:,1] 
    return ginic(a, p) / ginic(a, a)

#models
logit=LogisticRegression(max_iter=3000)
lg=BaggingClassifier(base_estimator=logit,n_estimators=100,max_features=.7,max_samples=.8)
naive=GaussianNB()
randomforest = RandomForestClassifier(n_estimators=2500, min_samples_leaf=200,max_leaf_nodes=600,random_state=0, n_jobs=-1,class_weight='balanced',oob_score=True)

#cross validation
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)
for train_ix,test_ix in cv.split(porto,target):
  X_train, X_test = porto.iloc[train_ix,:], porto.iloc[test_ix,:]
  y_train, y_test = target[train_ix],target[test_ix]
  naive.fit(X_train,y_train)
  y_prob=naive.predict_proba(X_test)
  score=gini_normalizedc(y_test,y_prob)
  print(score)

model=naive.fit(porto,target)

testdata=pd.read_csv('/content/drive/My Drive/porto-seguro/test.csv')

id=testdata['id']

testdata.drop(columns=['ps_car_03_cat','id'],axis=1,inplace=True)
testdata.drop(columns=lis,axis=1,inplace=True)

predictionprob=model.predict_proba(testdata)
predictionprob

predictionfinal=pd.DataFrame(id,columns=['id'])
predictionfinal['target']=predictionprob[:,1]
predictionfinal

predictionfinal.to_csv('prediction.csv',index=False)