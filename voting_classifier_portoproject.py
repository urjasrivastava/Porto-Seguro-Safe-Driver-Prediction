# -*- coding: utf-8 -*-
"""voting-classifier-portoproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sRdTsLKiyyiQqhLZQYzO6b-Ak5Kuk0Wf
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import transforms
import seaborn as sns
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from catboost import CatBoostClassifier
from sklearn.preprocessing import OneHotEncoder 
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.naive_bayes import GaussianNB
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFECV
from sklearn import  linear_model
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import VotingClassifier
# %matplotlib inline

porto=pd.read_csv("/content/drive/My Drive/porto-seguro/train.csv");

porto.target.plot.hist()

portowtnan=porto.replace(-1,np.nan)

portowtnan['missingmark']=portowtnan.isnull().sum(1) #add missing mark

portowtnan.replace(np.nan,-1,inplace=True)

portowtnan.info()

portowtnan.drop(columns=['ps_car_03_cat','id'],axis=1,inplace=True) #drop columns with more than 50% null values
target=portowtnan['target']
portowtnan.drop(columns=['target'],axis=1,inplace=True)
cols=portowtnan.columns

portowtnan

portoclean=portowtnan
portoclean.head()

portocalc=[cols for cols in portoclean.columns if cols.find("calc")!=-1 and cols.find("13")==-1] #calc_13 is found to be boosting our prediction accuracy, therefore we keep it
portocalc

portoclean.drop(portocalc,axis=1,inplace=True)
portoclean

portobin=[cols for cols in portoclean.columns if cols.find("bin")!=-1 ] #dataset seggregation
portocat=[cols for cols in portoclean.columns if cols.find("cat")!=-1 ]
portonormal=[cols for cols in portoclean.columns if cols.find("bin")==-1 & cols.find("cat")==-1 ]

portobin

prbin=portoclean.loc[:,portobin]
prbin

selected=portobin
selected

portocat

prcat=portoclean.loc[:,portocat]
prcat

"""#Two new Features - car,ind"""

elem=[cols for cols in prcat.columns if cols.find("car")!=-1 ]
df=pd.DataFrame()
df['target']=target
df['car']=np.zeros(len(prcat))
for col in elem:
  df['car']=df['car']+prcat[col]**.5
df.groupby('car')['target'].sum().plot.bar() #this features is very impactful and boosted our prediciton acurracy by a huge margin

elem=[cols for cols in portoclean.columns if cols.find("ind")!=-1 ]
df=pd.DataFrame()
df['target']=target
df['ind']=np.zeros(len(prcat))
for col in elem:
  df['ind']=df['ind']+portoclean[col]
df['ind']=df['ind']**2
df.groupby('ind')['target'].sum().plot.bar() #another great addition to our features, also boosted our leaderboard score

def calculatefreq(selectcat,df): #frequency encoding
  for col in selectcat:
    newcol='%s_freq'%(col)    
    m=portoclean[col].astype('category').value_counts(dropna=False).to_dict()
    df[newcol]=df[col].map(m)
  elem=[cols for cols in selectcat if cols.find("car")!=-1 ]   #adding logic to calculate car column
  df['car']=np.zeros(len(df))
  for col in elem:
    df['car']=df['car']+df[col]**.5
  return(df)

selectcat=portocat

prfreq=calculatefreq(selectcat,prcat)
prfreq

prcat=prfreq
selectcat=prfreq.columns

prcat.columns

portonormal

prnor=portoclean.loc[:,portonormal]
prnor

selectednorm=portonormal

def featureengineering(df): #feature engineering as before
  top=['ps_car_14','ps_car_15','ps_reg_03','ps_ind_15']
  i=0
  for col in top:
    new='%d'%i
    new1='%d_interac'%i   
    m=portoclean.groupby(col)['ps_car_13'].mean().to_dict()      
    df[new]=df[col].map(m)
    df[new1]=(df['ps_car_13']*df[col])   
    i=i+1  
  tenco=pd.DataFrame()
  tenco['target']=target
  cat=['ps_car_11_cat','ps_car_06_cat']
  for col in cat:
   tenco[col]=prcat[col]
   m=(tenco.groupby(col)['target'].mean()).to_dict()
   newcol='%s_new'%col
   df[newcol]=df[col].map(m)
  listcol=[cols for cols in portoclean.columns if cols.find("ind")!=-1 ] #logic for calculating column ind
  df['ind']=np.zeros(len(df))
  for col in listcol:
    df['ind']=df['ind']+df[col]  
  df['ind']=df['ind']**2
  return df

features_train=pd.concat([prbin,prcat,prnor],join='outer',axis=1)
selectedfeatures=pd.concat([pd.Series(selected),pd.Series(selectcat),pd.Series(selectednorm)],axis=0,ignore_index=True)

features_train=featureengineering(features_train.copy())
features_train

def ginic(actual, pred):
    actual = np.asarray(actual) 
    n = len(actual)
    a_s = actual[np.argsort(pred)]
    a_c = a_s.cumsum()
    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0
    return giniSum / n
 
def gini_normalizedc(a, p):
    if p.ndim == 2:
        p = p[:,1] 
    return ginic(a, p) / ginic(a, a)

xgb1 = xgb.XGBClassifier(tree_method='gpu_hist',learning_rate=0.01, 
                            n_estimators=1600, 
                            max_depth=6,                            
                            gamma=0.5,
                            subsample=.8,
                            colsample_bytree=0.3,
                            objective= 'binary:logistic',                            
                            scale_pos_weight=1,
                            reg_alpha = 1,
                            reg_lambda = 3,
                            n_jobs=-1,
                            seed=49)

#0.27854 0.29230  #also proved to be our best model on the private leadearboard
lg=lgb.LGBMClassifier(boosting_type='gbdt',learning_rate=.01,max_depth=6,n_estimators=1400,colsample_bytree=.3,reg_alpha=3, reg_lambda=3)

lgb2=lgb.LGBMClassifier(colsample_bytree=0.30763246783148277,reg_alpha=5.840063271783509,tree_method='gpu_hist',
learning_rate=0.015661173689900194,n_estimators=1200,
reg_lambda=2.797801550526486,subsample= 0.5986289358690637,max_depth=7)

model = CatBoostClassifier(iterations=2500,
learning_rate=0.01,
depth=8,
)

estimators = [('xgb',xgb1),('lgb',lg),('cat',model),('lgb2',lgb2)]
clf =VotingClassifier(estimators=estimators,voting='soft')
#0.27784 0.29312 best performance of voting classifier

cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)
for train_ix,test_ix in cv.split(features_train,target):
  X_train, X_test = features_train.iloc[train_ix,:], features_train.iloc[test_ix,:]
  y_train, y_test = target[train_ix],target[test_ix]
  lgb2.fit(X_train,y_train)
  y_prob=lgb2.predict_proba(X_test)
  score=gini_normalizedc(y_test,y_prob)
  print(score)

modelgb=clf.fit(features_train,target)

fig, ax = plt.subplots(1,1,figsize=(15,15)) #our engineered features right at the top 
lgb.plot_importance(lg,ax=ax)

testdata=pd.read_csv('/content/drive/My Drive/porto-seguro/test.csv')

id=testdata['id']

testdata.replace(-1,np.nan,inplace=True)

testdata['missingmark']=testdata.isnull().sum(1)
testdata.replace(np.nan,-1,inplace=True)

testdata.drop(columns=portocalc,axis=1,inplace=True)

testdata.drop(columns=['id','ps_car_03_cat'],axis=1,inplace=True)

testdata.head()

testbin=testdata.loc[:,selected].copy()
testbin

testnor=testdata.loc[:,selectednorm].copy()
testnor

testcatfreq=calculatefreq(portocat,testdata)

testcat=testcatfreq.loc[:,selectcat].copy()
testcat

testcat.nunique()

test1=pd.concat([testbin,testcat,testnor],join='outer',axis=1)
test1

test1=featureengineering(test1.copy())

test1

predictionprob1=pd.DataFrame(modelgb.predict_proba(test1))
predictionprob1

predictionfinal=pd.DataFrame(id,columns=['id'])
predictionfinal['target']=predictionprob1.iloc[:,1]
predictionfinal

predictionfinal.to_csv('prediction.csv',index=False)