# -*- coding: utf-8 -*-
"""porto-feature-selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ywIShpVeH7Tn4vqAiLuhChF6LCr08L1b
"""

# Commented out IPython magic to ensure Python compatibility.
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
import matplotlib.pyplot as plt
from matplotlib import transforms
import seaborn as sns
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder 
from sklearn.impute import SimpleImputer
# %matplotlib inline
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
                
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFECV
from sklearn import  linear_model

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb
import xgboost as xgb
# %matplotlib inline

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""##Summary of different techniques we used to feature selection and extraction"""

data=pd.read_csv('/kaggle/input/porto-seguro-new/train.csv');
data_test=pd.read_csv('/kaggle/input/porto-seguro-new/test.csv');

data.head()

data.describe()

data.shape

data.nunique()

data.info()

"""Cleaning the data"""

data_copy=data.copy()

data_copy=data_copy.replace(-1,np.nan)

data_copy.isnull().sum()

data_copy.target.plot.hist()

data_copy.drop(['ps_car_03_cat'],axis=1,inplace=True)

target=data_copy['target']

"""#Finding the percentage of Null-Values"""

features_miss= []

for i in data.columns:
    missing = data[data[i] == -1][i].count()
    if missing > 0:
        features_miss.append(i)
        miss_percent = missing/data.shape[0]
        #miss_percent=miss_percent*100
        if missing>288180:
            print("Highest missing values in {}".format(i))
            print('Total missing values in {} is {}'.format(i, missing))
            print("percentage of missing values is ({:.3%})".format(miss_percent))

data_copy=data_copy.replace(-1,np.nan)
cols=data_copy.columns

id_train=data_copy['id']
data_copy.drop(columns=['id'],axis=1,inplace=True)

cols=data_copy.columns

"""#Imputation technique used- mode"""

imp=SimpleImputer(missing_values=np.nan, strategy='most_frequent') #mode imputation
imp.fit(data_copy)
data_copy=pd.DataFrame(imp.transform(data_copy),columns=cols)
data_copy.head()

portobin=[cols for cols in data_copy.columns if cols.find("bin")!=-1 ]
portocat=[cols for cols in data_copy.columns if cols.find("cat")!=-1 ]
portonormal=[cols for cols in data_copy.columns if cols.find("bin")==-1 & cols.find("cat")==-1 ]

print(portobin)
print("****************************")
print(portocat)
print("****************************")
print(portonormal)
print("****************************")

prbin=data_copy.loc[:,portobin]
prbin.head()

"""Variance thresholding for binary features"""

thresholder = VarianceThreshold(threshold=(.90 * (1 - .90)))
prbin=thresholder.fit_transform(prbin)
prbin.shape

portobin

selected=pd.Series(portobin)[thresholder.get_support()] #selected features
selected

prbin=pd.DataFrame(prbin,columns=selected) #selected binary features
prbin

prcat=data_copy.loc[:,portocat]
prcat

"""Chi2 statistics for categorical variables"""

prcat=prcat.astype(int)
chi2_selector = SelectKBest(chi2, k=10)
prcat = chi2_selector.fit_transform(prcat,data_copy["target"])
prcat.shape

selectedcat=pd.Series(portocat)[chi2_selector.get_support()] #selected features
selectedcat

(chi2_selector.scores_)

prcat=pd.DataFrame(prcat,columns=selectedcat) 
prcat

portonormal.remove('target')
portonormal

prnor=data_copy.loc[:,portonormal]
prnor

"""Recursive feature elimination for normal features"""

logit = linear_model.LogisticRegression(max_iter=3000,class_weight='balanced',penalty='l2')
# Recursively eliminate features
rfecv = RFECV(estimator=logit, step=1, scoring="roc_auc")
rfecv.fit(prnor, data_copy["target"])
rfecv.transform(prnor)

prnor.columns[rfecv.get_support()] #selected features

(rfecv.grid_scores_)*2-1

print('Optimal number of features: {}'.format(rfecv.n_features_))

selectednorm=pd.Series(['ps_ind_01', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02',
       'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14',
       'ps_car_15'])
selectednorm

prnormal=prnor.loc[:,selectednorm]
prnormal

train_data_new=pd.concat([prbin,prnormal,prcat],join='outer',axis=1)
selectedfeatures=pd.concat([selected,pd.Series(selectedcat),selectednorm],axis=0,ignore_index=True)

selectedfeatures

"""Pca for feature extraction"""

data_pca=data_copy.copy()

scaler=StandardScaler()
scaler.fit(data_pca)

StandardScaler(copy=True,with_mean=True,with_std=True)

data_pca=scaler.transform(data_pca)
data_pca

from sklearn.decomposition import PCA

pca=PCA(n_components=55)
pca.fit(data_pca)

resultant_pca=pca.transform(data_pca)

data_pca.shape

resultant_pca.shape

resultant_pca

plt.scatter(resultant_pca[:,0],resultant_pca[:,1],c=data_copy['target'])
plt.xlabel('x - axis')
plt.ylabel('y - axis')

plt.scatter(data_pca[:,0],data_pca[:,1],c=data_copy['target'])
plt.xlabel('x - axis')
plt.ylabel('y - axis')